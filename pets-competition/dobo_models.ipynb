{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pydot\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals.six import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import score_solution\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('all_train_data_V2.csv')\n",
    "adoptionSpeed = df['AdoptionSpeed']\n",
    "del df['AdoptionSpeed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by the scale function.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by the scale function.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data, test_data, adoptionSpeed, correct_ans = train_test_split(df, adoptionSpeed, test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "del train_data[\"Name\"]\n",
    "del train_data[\"RescuerID\"]\n",
    "del train_data[\"Description\"]\n",
    "del train_data[\"PetID\"]\n",
    "\n",
    "\n",
    "del test_data[\"Name\"]\n",
    "del test_data[\"RescuerID\"]\n",
    "del test_data[\"Description\"]\n",
    "del test_data[\"PetID\"]\n",
    "\n",
    "train_data = train_data.fillna(0)\n",
    "test_data = test_data.fillna(0)\n",
    "train_data = preprocessing.scale(train_data)\n",
    "test_data = preprocessing.scale(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13493, 53)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kappa_eval = make_scorer(cohen_kappa_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: object of type <class 'float'> cannot be safely interpreted as an integer.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: object of type <class 'float'> cannot be safely interpreted as an integer.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': [],\n",
       " 'max_depth': [3],\n",
       " 'subsample': [0.8, 0.9, 1],\n",
       " 'colsample_bytree': [],\n",
       " 'gamma': [0, 1, 5],\n",
       " 'n_estimators': [300, 416, 533, 650, 766, 883, 1000]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 300, stop = 1000, num = 7)]\n",
    "# Number of features to consider at every split\n",
    "max_depths = [int(x) for x in np.linspace(3, 6, num = 1)]\n",
    "learning_rate = [float(x) for x in np.linspace(0.01, 0.1, num = 0.02)]\n",
    "subsample = [0.8, 0.9, 1]\n",
    "# Minimum number of samples required at each leaf node\n",
    "colsample_bytree = [float(x) for x in np.linspace(0.3, 0.8, num = 0.2)]\n",
    "# Method of selecting samples for training each tree\n",
    "gamma = [0, 1, 5]\n",
    "\n",
    "\n",
    "random_grid = {\n",
    "    'learning_rate': learning_rate,\n",
    "    'max_depth': max_depths,\n",
    "    'subsample': subsample,\n",
    "    'colsample_bytree': colsample_bytree,\n",
    "    'gamma': gamma,\n",
    "    'n_estimators': n_estimators}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random_grid\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomizedSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a80a1e657adc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mxgb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m xgb_random = RandomizedSearchCV(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m      \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkappa_eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomizedSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "xgb_model = XGBClassifier()\n",
    "xgb_random = RandomizedSearchCV(\n",
    "    estimator = xgb_model, param_distributions = random_grid,\n",
    "     n_iter = 100, cv = 3, verbose=2, random_state=13,\n",
    "     n_jobs = 4, scoring=kappa_eval\n",
    ")\n",
    "# Fit the random search model\n",
    "xgb_random.fit(train_data, adoptionSpeed)\n",
    "best_random = rf_random.best_estimator_\n",
    "print(best_random)\n",
    "ans = best_random.predict(test_data)\n",
    "best_random\n",
    "kappa_score = cohen_kappa_score(ans,correct_ans)\n",
    "kappa_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1958357785617102"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.5, gamma=0, learning_rate=0.03, max_delta_step=0,\n",
    "       max_depth=3, min_child_weight=1, missing=None, n_estimators=650,\n",
    "       n_jobs=4, nthread=None, objective='multi:softprob', random_state=13,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=0.8)\n",
    "\n",
    "# Fit the random search model\n",
    "xgb_model.fit(train_data, adoptionSpeed)\n",
    "ans = xgb_model.predict(test_data)\n",
    "\n",
    "kappa_score = cohen_kappa_score(ans,correct_ans)\n",
    "kappa_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2159624046463089"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "bagging_model = BaggingClassifier(DecisionTreeClassifier(), n_estimators = 500,\n",
    "                            max_samples=0.5, max_features=0.5)\n",
    "\n",
    "\n",
    "bagging_model.fit(train_data, adoptionSpeed)\n",
    "ans = bagging_model.predict(test_data)\n",
    "kappa_score = score_solution.kappa(ans,correct_ans)\n",
    "kappa_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed: 35.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=2, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=2, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 1, stop = 1000, num = 7)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['log2', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depths = [int(x) for x in np.linspace(20, 60, num = 5)]\n",
    "max_depths.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_splits = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leafs = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstraps = [True, False]\n",
    "\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depths,\n",
    "               'min_samples_split': min_samples_splits,\n",
    "               'min_samples_leaf': min_samples_leafs,\n",
    "               'bootstrap': bootstraps}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "kappa_eval = make_scorer(cohen_kappa_score)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator = rf, param_distributions = random_grid,\n",
    "     n_iter = 100, cv = 3, verbose=2, random_state=13,\n",
    "     n_jobs = 4, scoring=kappa_eval\n",
    ")\n",
    "# Fit the random search model\n",
    "rf_random.fit(train_data, adoptionSpeed)\n",
    "best_random = rf_random.best_estimator_\n",
    "print(best_random)\n",
    "ans = best_random.predict(test_data)\n",
    "\n",
    "kappa_score = score_solution.kappa(ans,correct_ans)\n",
    "kappa_score\n",
    "\n",
    "best_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23556388352643143"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kappa_score\n",
    "\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "# extra_model = ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "#             max_depth=40, max_features='log2', max_leaf_nodes=None,\n",
    "#             min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#             min_samples_leaf=2, min_samples_split=6,\n",
    "#             min_weight_fraction_leaf=0.0, n_estimators=630, n_jobs=None,\n",
    "#             oob_score=False, random_state=None, verbose=0,\n",
    "#             warm_start=False)\n",
    "\n",
    "# extra_model.fit(train_data, adoptionSpeed)\n",
    "\n",
    "# ans = extra_model.predict(test_data)\n",
    "\n",
    "# kappa_score = score_solution.kappa(ans,correct_ans)\n",
    "# kappa_score\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19660782051518066"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "agb_model = AdaBoostClassifier(learning_rate=0.1, n_estimators=1000,\n",
    "                           algorithm='SAMME.R', base_estimator=DecisionTreeClassifier(max_depth=2))\n",
    "\n",
    "\n",
    "agb_model.fit(train_data, adoptionSpeed)\n",
    "\n",
    "ans = agb_model.predict(test_data)\n",
    "\n",
    "kappa_score = score_solution.kappa(ans,correct_ans)\n",
    "kappa_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2061097838449416"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc_model = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.1,\n",
    "     max_depth=2)\n",
    "\n",
    "gbc_model.fit(train_data, adoptionSpeed)\n",
    "\n",
    "ans = gbc_model.predict(test_data)\n",
    "\n",
    "kappa_score = score_solution.kappa(ans,correct_ans)\n",
    "kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21475939445614123"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "gbc_model = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.1,\n",
    "     max_depth=2)\n",
    "agb_model = AdaBoostClassifier(learning_rate=0.1, n_estimators=1000,\n",
    "                           algorithm='SAMME.R', base_estimator=DecisionTreeClassifier(max_depth=2))\n",
    "extra_model = ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=2, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\n",
    "rf_model = RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=2, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "bagging_model = BaggingClassifier(DecisionTreeClassifier(), n_estimators = 1000,\n",
    "                            max_samples=0.5, max_features=0.5)\n",
    "\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.5, gamma=0, learning_rate=0.03, max_delta_step=0,\n",
    "       max_depth=3, min_child_weight=1, missing=None, n_estimators=650,\n",
    "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=0.8)\n",
    "\n",
    "classifiers = [\n",
    "    ('gbc', gbc_model), ('agb', agb_model),\n",
    "    ('extra', extra_model), ('rf', rf_model),\n",
    "    ('bagging', bagging_model), (\"xgb\", xgb_model)\n",
    "]\n",
    "\n",
    "voting_model = VotingClassifier(estimators=classifiers, voting='hard')\n",
    "\n",
    "voting_model.fit(train_data, adoptionSpeed)\n",
    "\n",
    "ans = voting_model.predict(test_data)\n",
    "\n",
    "kappa_score = score_solution.kappa(ans,correct_ans)\n",
    "kappa_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "12143/12143 [==============================] - 15s 1ms/step - loss: 1.5355 - acc: 0.2732\n",
      "Epoch 1/1\n",
      "12143/12143 [==============================] - 17s 1ms/step - loss: 1.4844 - acc: 0.2777\n",
      "Epoch 1/1\n",
      "12143/12143 [==============================] - 15s 1ms/step - loss: 1.4997 - acc: 0.2694\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 14s 1ms/step - loss: 1.4863 - acc: 0.2784\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 16s 1ms/step - loss: 1.4861 - acc: 0.2747\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 20s 2ms/step - loss: 1.4927 - acc: 0.2791\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 18s 2ms/step - loss: 1.4824 - acc: 0.2779\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 10s 828us/step - loss: 1.4845 - acc: 0.2750\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 11s 903us/step - loss: 1.4826 - acc: 0.2754\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 14s 1ms/step - loss: 1.4873 - acc: 0.2741\n",
      "Epoch 1/1\n",
      "12143/12143 [==============================] - 15s 1ms/step - loss: 1.5006 - acc: 0.2708\n",
      "Epoch 1/1\n",
      "12143/12143 [==============================] - 14s 1ms/step - loss: 1.4946 - acc: 0.2752\n",
      "Epoch 1/1\n",
      "12143/12143 [==============================] - 12s 979us/step - loss: 1.5176 - acc: 0.2748\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 18s 2ms/step - loss: 1.4996 - acc: 0.2695\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 16s 1ms/step - loss: 1.4893 - acc: 0.2758\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 14s 1ms/step - loss: 1.5368 - acc: 0.2727\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 13s 1ms/step - loss: 1.5948 - acc: 0.2717\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 13s 1ms/step - loss: 1.4987 - acc: 0.2763\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 13s 1ms/step - loss: 1.5062 - acc: 0.2729\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 13s 1ms/step - loss: 1.7506 - acc: 0.2680\n",
      "Epoch 1/2\n",
      "12143/12143 [==============================] - 14s 1ms/step - loss: 1.4855 - acc: 0.2760\n",
      "Epoch 2/2\n",
      "12143/12143 [==============================] - 15s 1ms/step - loss: 1.4671 - acc: 0.2770\n",
      "Epoch 1/2\n",
      "12143/12143 [==============================] - 17s 1ms/step - loss: 1.4839 - acc: 0.2715\n",
      "Epoch 2/2\n",
      "12143/12143 [==============================] - 18s 1ms/step - loss: 1.4660 - acc: 0.2786\n",
      "Epoch 1/2\n",
      "12143/12143 [==============================] - 16s 1ms/step - loss: 12.5687 - acc: 0.2202\n",
      "Epoch 2/2\n",
      "12143/12143 [==============================] - 14s 1ms/step - loss: 12.5687 - acc: 0.2202\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 17s 1ms/step - loss: 1.4934 - acc: 0.2766\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 16s 1ms/step - loss: 1.4657 - acc: 0.2750\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 17s 1ms/step - loss: 1.4802 - acc: 0.2764\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 18s 2ms/step - loss: 1.4644 - acc: 0.2826\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 17s 1ms/step - loss: 1.4851 - acc: 0.2737\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 16s 1ms/step - loss: 1.4668 - acc: 0.2750\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 13s 1ms/step - loss: 1.4933 - acc: 0.2787\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 13s 1ms/step - loss: 1.4654 - acc: 0.2764\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 14s 1ms/step - loss: 1.4839 - acc: 0.2708\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 13s 1ms/step - loss: 1.4652 - acc: 0.2775\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 19s 2ms/step - loss: 1.5082 - acc: 0.2703\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 16s 1ms/step - loss: 1.4658 - acc: 0.2748\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 17s 1ms/step - loss: 1.4851 - acc: 0.2724\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 21s 2ms/step - loss: 1.4659 - acc: 0.2758\n",
      "Epoch 1/2\n",
      "12143/12143 [==============================] - 21s 2ms/step - loss: 1.5159 - acc: 0.2795\n",
      "Epoch 2/2\n",
      "12143/12143 [==============================] - 19s 2ms/step - loss: 1.4824 - acc: 0.2664\n",
      "Epoch 1/2\n",
      "12143/12143 [==============================] - 18s 2ms/step - loss: 1.5109 - acc: 0.2741\n",
      "Epoch 2/2\n",
      "12143/12143 [==============================] - 22s 2ms/step - loss: 1.5133 - acc: 0.2844\n",
      "Epoch 1/2\n",
      "12143/12143 [==============================] - 24s 2ms/step - loss: 1.5230 - acc: 0.2675\n",
      "Epoch 2/2\n",
      "12143/12143 [==============================] - 21s 2ms/step - loss: 1.4735 - acc: 0.2748\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 25s 2ms/step - loss: 1.5124 - acc: 0.2755\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 22s 2ms/step - loss: 1.4795 - acc: 0.2786\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 19s 2ms/step - loss: 1.5147 - acc: 0.2754\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 20s 2ms/step - loss: 1.4910 - acc: 0.2740\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 19s 2ms/step - loss: 1.5105 - acc: 0.2778\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 27s 2ms/step - loss: 1.4871 - acc: 0.2718\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 17s 1ms/step - loss: 1.4855 - acc: 0.2750\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 20s 2ms/step - loss: 1.4888 - acc: 0.2768\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 23s 2ms/step - loss: 1.5336 - acc: 0.2720\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 20s 2ms/step - loss: 1.4816 - acc: 0.2795\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 22s 2ms/step - loss: 1.5313 - acc: 0.2720\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 20s 2ms/step - loss: 1.4787 - acc: 0.2757\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 16s 1ms/step - loss: 1.5144 - acc: 0.2736\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 14s 1ms/step - loss: 1.4824 - acc: 0.2770\n",
      "Epoch 1/1\n",
      "12143/12143 [==============================] - 2s 148us/step - loss: 1.5683 - acc: 0.2766\n",
      "Epoch 1/1\n",
      "12143/12143 [==============================] - 2s 159us/step - loss: 1.7182 - acc: 0.2728\n",
      "Epoch 1/1\n",
      "12143/12143 [==============================] - 2s 157us/step - loss: 1.6609 - acc: 0.2761\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 2s 165us/step - loss: 1.5828 - acc: 0.2840\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 2s 172us/step - loss: 1.5327 - acc: 0.2865\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 2s 180us/step - loss: 1.5550 - acc: 0.2832\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 2s 179us/step - loss: 1.5774 - acc: 0.2679\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 2s 179us/step - loss: 1.6080 - acc: 0.2811\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 2s 180us/step - loss: 1.5375 - acc: 0.2740\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 2s 186us/step - loss: 1.5331 - acc: 0.2670\n",
      "Epoch 1/1\n",
      "12143/12143 [==============================] - 2s 184us/step - loss: 1.8371 - acc: 0.2625\n",
      "Epoch 1/1\n",
      "12143/12143 [==============================] - 2s 192us/step - loss: 1.6164 - acc: 0.2690\n",
      "Epoch 1/1\n",
      "12143/12143 [==============================] - 2s 192us/step - loss: 1.5846 - acc: 0.2583\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 2s 200us/step - loss: 1.5257 - acc: 0.2680\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 2s 200us/step - loss: 1.5454 - acc: 0.2758\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 2s 203us/step - loss: 1.6486 - acc: 0.2567\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 3s 209us/step - loss: 1.6085 - acc: 0.2822\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 3s 208us/step - loss: 1.4782 - acc: 0.2754\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 3s 214us/step - loss: 1.6329 - acc: 0.2769\n",
      "Epoch 1/1\n",
      "12144/12144 [==============================] - 3s 214us/step - loss: 1.4865 - acc: 0.2798\n",
      "Epoch 1/2\n",
      "12143/12143 [==============================] - 3s 238us/step - loss: 1.5442 - acc: 0.2784\n",
      "Epoch 2/2\n",
      "12143/12143 [==============================] - 0s 23us/step - loss: 1.4714 - acc: 0.2707\n",
      "Epoch 1/2\n",
      "12143/12143 [==============================] - 3s 235us/step - loss: 1.5432 - acc: 0.2783\n",
      "Epoch 2/2\n",
      "12143/12143 [==============================] - 0s 18us/step - loss: 1.4592 - acc: 0.2974\n",
      "Epoch 1/2\n",
      "12143/12143 [==============================] - 3s 241us/step - loss: 1.4991 - acc: 0.2624\n",
      "Epoch 2/2\n",
      "12143/12143 [==============================] - 0s 21us/step - loss: 1.4596 - acc: 0.2839\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 3s 256us/step - loss: 1.5532 - acc: 0.2652\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 0s 20us/step - loss: 1.4562 - acc: 0.2941\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 3s 251us/step - loss: 1.7990 - acc: 0.2775\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 0s 19us/step - loss: 1.4703 - acc: 0.2933\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 3s 251us/step - loss: 1.7645 - acc: 0.2612\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 0s 23us/step - loss: 1.4574 - acc: 0.2949\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 3s 258us/step - loss: 1.5134 - acc: 0.2894\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 0s 19us/step - loss: 1.4587 - acc: 0.3051\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 3s 258us/step - loss: 1.6282 - acc: 0.2736\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 0s 19us/step - loss: 1.5668 - acc: 0.2788\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 3s 261us/step - loss: 1.6054 - acc: 0.2838\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 0s 20us/step - loss: 1.4587 - acc: 0.2980\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 3s 271us/step - loss: 1.5948 - acc: 0.2609\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 0s 25us/step - loss: 1.4601 - acc: 0.2996\n",
      "Epoch 1/2\n",
      "12143/12143 [==============================] - 3s 270us/step - loss: 1.5036 - acc: 0.2864\n",
      "Epoch 2/2\n",
      "12143/12143 [==============================] - 0s 23us/step - loss: 1.4654 - acc: 0.2961\n",
      "Epoch 1/2\n",
      "12143/12143 [==============================] - 4s 290us/step - loss: 1.5311 - acc: 0.2664\n",
      "Epoch 2/2\n",
      "12143/12143 [==============================] - 0s 26us/step - loss: 1.4597 - acc: 0.2869\n",
      "Epoch 1/2\n",
      "12143/12143 [==============================] - 3s 282us/step - loss: 1.5831 - acc: 0.2756\n",
      "Epoch 2/2\n",
      "12143/12143 [==============================] - 0s 22us/step - loss: 1.4749 - acc: 0.2835\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 3s 279us/step - loss: 1.4743 - acc: 0.2803\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 0s 20us/step - loss: 1.4553 - acc: 0.3011\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 3s 286us/step - loss: 1.5509 - acc: 0.2698\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 0s 24us/step - loss: 1.4719 - acc: 0.2918\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 4s 291us/step - loss: 1.5163 - acc: 0.2636\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 0s 25us/step - loss: 1.4710 - acc: 0.2737\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 4s 294us/step - loss: 1.6667 - acc: 0.2687\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 0s 22us/step - loss: 1.4892 - acc: 0.2848\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 4s 304us/step - loss: 1.5468 - acc: 0.2647\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 0s 24us/step - loss: 1.4726 - acc: 0.2778\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 4s 302us/step - loss: 1.5429 - acc: 0.2708\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 0s 25us/step - loss: 1.4784 - acc: 0.2731\n",
      "Epoch 1/2\n",
      "12144/12144 [==============================] - 4s 305us/step - loss: 1.4785 - acc: 0.2925\n",
      "Epoch 2/2\n",
      "12144/12144 [==============================] - 0s 22us/step - loss: 1.4579 - acc: 0.2996\n",
      "Epoch 1/2\n",
      "13493/13493 [==============================] - 4s 276us/step - loss: 1.6954 - acc: 0.2721\n",
      "Epoch 2/2\n",
      "13493/13493 [==============================] - 0s 24us/step - loss: 1.4789 - acc: 0.2865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.wrappers.scikit_learn.KerasClassifier at 0x7fa6aef23e10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "\n",
    "def build_classifier(optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = 10, kernel_initializer = 'uniform', input_dim = 56, activation = 'relu'))\n",
    "    model.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    model.add(Dense(units = 5, kernel_initializer = 'uniform', activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "kera_classifier = KerasClassifier(build_fn = build_classifier)\n",
    "\n",
    "parameters = {'batch_size':[1,100],\n",
    "              'epochs':[1,2],\n",
    "              'optimizer':['adam','rmsprop']}\n",
    "grid_search = GridSearchCV(estimator = kera_classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10)\n",
    "\n",
    "grid_search = grid_search.fit(train_data, adoptionSpeed)\n",
    "kera_model = grid_search.best_estimator_\n",
    "kera_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data, combine_data, adoptionSpeed, combine_ans = train_test_split(\n",
    "    train_data, adoptionSpeed,\n",
    "    test_size=0.15\n",
    ")\n",
    "\n",
    "\n",
    "kappa_eval = make_scorer(cohen_kappa_score)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 500, stop = 1400, num = 7)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['log2', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depths = [int(x) for x in np.linspace(30, 70, num = 5)]\n",
    "max_depths.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_splits = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leafs = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstraps = [True, False]\n",
    "\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depths,\n",
    "               'min_samples_split': min_samples_splits,\n",
    "               'min_samples_leaf': min_samples_leafs,\n",
    "               'bootstrap': bootstraps}\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator = rf, param_distributions = random_grid,\n",
    "     n_iter = 100, cv = 5, verbose=2, random_state=13,\n",
    "     n_jobs = 4, scoring=kappa_eval\n",
    ")\n",
    "# Fit the random search model\n",
    "rf_random.fit(train_data, adoptionSpeed)\n",
    "best_rf = rf_random.best_estimator_\n",
    "ans = best_rf.predict(combine_data)\n",
    "\n",
    "importances = best_rf.feature_importances_\n",
    "\n",
    "sorted_futures = np.argsort(importances)[::-1]\n",
    "\n",
    "\n",
    "feature_train = train_data.iloc[:, sorted_futures[0]]\n",
    "feature_test = combine_data.iloc[:, sorted_futures[0]]\n",
    "feature_submission = test_data.iloc[:, sorted_futures[0]]\n",
    "\n",
    "for i in range(1, 31):\n",
    "    subset1 = train_data.iloc[ : ,sorted_futures[i]]\n",
    "    subset2 = combine_data.iloc[ : ,sorted_futures[i]]\n",
    "    subset3 = test_data.iloc[:, sorted_futures[i]]\n",
    "    feature_train = pd.concat([feature_train, subset1], axis=1)\n",
    "    feature_test = pd.concat([feature_test, subset2], axis=1)\n",
    "    feature_submission = pd.concat([feature_submission, subset3], axis=1)\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('sorted_futures.csv', 'wb') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(sorted_futures)\n",
    "    \n",
    "sorted_futures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
