{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network\n",
    " \n",
    "# Installing Theano\n",
    "# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n",
    " \n",
    "# Installing Tensorflow\n",
    "# Install Tensorflow from the website: https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html\n",
    " \n",
    "# Installing Keras\n",
    "# pip install --upgrade keras\n",
    "\n",
    "#IMAGES_PATH = 'C:/Users/madad/Documents/dataset/images/resized/'\n",
    "RESIZED_IMAGES_PATH = 'C:/Users/madad/Documents/dataset/images/resized/'\n",
    "CSV_PATH = 'C:/Users/madad/Documents/dataset/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Part 1 - Building the Models\n",
    " \n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Multilayer Perceptron\n",
    "#-----------------------------------------------------------------------------\n",
    "def create_mlp(dim, regress=False):\n",
    "    # define our MLP network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "\n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "    # return our model\n",
    "    return model\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Regression-based CNN\n",
    "#-----------------------------------------------------------------------------\n",
    "def create_cnn(width, height, depth, filters=(16, 32, 64), regress=False):\n",
    "    # initialize the input shape and channel dimension, assuming\n",
    "    # TensorFlow/channels-last ordering\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1\n",
    "\n",
    "    # define the model input\n",
    "    inputs = Input(shape=inputShape)\n",
    "\n",
    "    # loop over the number of filters\n",
    "    for (i, f) in enumerate(filters):\n",
    "        # if this is the first CONV layer then set the input\n",
    "        # appropriately\n",
    "        if i == 0:\n",
    "            x = inputs\n",
    "\n",
    "        # Step 1 - Convolution\n",
    "        x = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
    "        \n",
    "        # Step 2 - RELU\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "        # Step 3 - BN\n",
    "        x = BatchNormalization(axis=chanDim)(x)\n",
    "        \n",
    "        # Step 4 - Pooling\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Step 5 - Flattening\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # Step 6 - FC layer\n",
    "    x = Dense(16)(x)\n",
    "    \n",
    "    # Step 7 - RELU\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    # Step 8 - BN\n",
    "    x = BatchNormalization(axis=chanDim)(x)\n",
    "    \n",
    "    # Step 9 - DROPOUT\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Step 10 - apply another FC layer, this one to match the number of nodes\n",
    "    # coming out of the MLP\n",
    "    x = Dense(4)(x)\n",
    "    \n",
    "    # Step 11 - RELU\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        x = Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "    # construct the CNN\n",
    "    model = Model(inputs, x)\n",
    "\n",
    "    # return the CNN\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "\n",
    "def resize_image(source_path, dest_path):\n",
    "    image = Image.open(source_path).convert('RGB')\n",
    "    resized_image = image.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "    # Uncomment to save to local directory \n",
    "    # Get image name\n",
    "    name=os.path.basename(source_path)\n",
    "    resized_image.save(dest_path + name, \"JPEG\")\n",
    "\n",
    "    # new_image size (64,64)\n",
    "    return np.asarray(resized_image)\n",
    "\n",
    "\n",
    "def read_image(path):\n",
    "    image = Image.open(path).convert('RGB')\n",
    "    return np.asarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------\n",
    "# Load data\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def process_csv(df):\n",
    "    new_df = pd.DataFrame() \n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        quantity = int(row['PhotoAmt'])\n",
    "        \n",
    "        if (not re.match(\"^[a-zA-Z0-9_]*$\", row['PetID'])):\n",
    "            print(row['PetID'])\n",
    "            continue\n",
    "    \n",
    "        petId = str(row['PetID'])\n",
    "        \n",
    "        for i in range(1, quantity+1):\n",
    "            new_row = row\n",
    "            new_row['PetID'] = petId + '-' + str(i)\n",
    "            \n",
    "            new_df = new_df.append([new_row],ignore_index=True)\n",
    "            \n",
    "    return new_df\n",
    "    \n",
    "def load_pet_attributes(inputPath):\n",
    "    # initialize the list of column names in the CSV file and then\n",
    "    # load it using Pandas\n",
    "    cols = [\"Type\", \"Name\", \"Age\", \"Breed1\", \"Breed2\", \"Gender\", \"Color1\", \"Color2\", \"Color3\", \"MaturitySize\", \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"Health\", \"Quantity\", \"Fee\", \"State\", \"RescuerID\", \"VideoAmt\", \"Description\", \"PetID\", \"PhotoAmt\", \"AdoptionSpeed\"]\n",
    "    df = pd.read_csv(inputPath, header=0, usecols=['PhotoAmt', 'PetID', 'AdoptionSpeed'], names=cols)\n",
    "    \n",
    "    df = process_csv(df)\n",
    "    \n",
    "    # return the data frame\n",
    "    return df\n",
    "\n",
    "def load_pet_attributes_test(inputPath):\n",
    "    # initialize the list of column names in the CSV file and then\n",
    "    # load it using Pandas\n",
    "    cols = [\"Type\", \"Name\", \"Age\", \"Breed1\", \"Breed2\", \"Gender\", \"Color1\", \"Color2\", \"Color3\", \"MaturitySize\", \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"Health\", \"Quantity\", \"Fee\", \"State\", \"RescuerID\", \"VideoAmt\", \"Description\", \"PetID\", \"PhotoAmt\"]\n",
    "    df = pd.read_csv(inputPath, header=0, usecols=['PhotoAmt', 'PetID'], names=cols)\n",
    "    \n",
    "    df = process_csv(df)\n",
    "    \n",
    "    # return the data frame\n",
    "    return df\n",
    "\n",
    "def load_pet_images(df, inputDir):    \n",
    "    # initialize images array \n",
    "    images = []\n",
    "\n",
    "    # loop over the csv rows\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        img_path = inputDir + row['PetID'] + '.jpg'\n",
    "    \n",
    "        img = read_image(img_path)\n",
    "        \n",
    "        # add the image to the set of images the network will be trained on\n",
    "        images.append(img)\n",
    "    \n",
    "    # return our set of images\n",
    "    return np.array(images)\n",
    "        \n",
    "def load_resize_pet_images(df, source, destination):    \n",
    "    # initialize images array \n",
    "    images = []\n",
    "    \n",
    "    # loop over the csv rows\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        img_source = source + row['PetID'] + '.jpg'\n",
    "    \n",
    "        resized = resize_image(img_source, destination)\n",
    "        \n",
    "        # add the image to the set of images the network will be trained on\n",
    "        images.append(resized)\n",
    "        \n",
    "    # return our set of images\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading pet features...\n",
      "[INFO] processed features\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------\n",
    "# Load pet features from csv\n",
    "#-----------------------------------------------------------------------------\n",
    "# construct the path to the train.csv file that contains information\n",
    "# on each pet in the dataset and then load the dataset\n",
    "print(\"[INFO] loading pet features...\")\n",
    "df = load_pet_attributes(CSV_PATH)\n",
    "print(\"[INFO] processed features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading pet images...\n",
      "[INFO] processed images\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------\n",
    "# Load the pet images\n",
    "#-----------------------------------------------------------------------------\n",
    "# scale the pixel intensities to the range [0, 1]\n",
    "print(\"[INFO] loading pet images...\")\n",
    "images = load_pet_images(df, RESIZED_IMAGES_PATH)\n",
    "images = images / 255.0\n",
    "print(\"[INFO] processed images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------\n",
    "# Split data into training and testing sets\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import locale\n",
    "\n",
    "# partition the data into training and testing splits using 90% of\n",
    "# the data for training and the remaining 10% for testing\n",
    "split = train_test_split(df, images, test_size=0.1, random_state=42)\n",
    "(trainAttrX, testAttrX, trainImagesX, testImagesX) = split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the pet adoption speed to the range [0, 1] (will lead to better\n",
    "# training and convergence)\n",
    "# the largest pet adoption value is 4 \n",
    "maxAdoption = 4\n",
    "trainY = trainAttrX[\"AdoptionSpeed\"] / maxAdoption\n",
    "testY = testAttrX[\"AdoptionSpeed\"] / maxAdoption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------\n",
    "# Define custom loss functions for regression in Keras \n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# root mean squared error (rmse) for regression\n",
    "def rmse(y_true, y_pred):\n",
    "    from keras import backend\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "# mean squared error (mse) for regression\n",
    "def mse(y_true, y_pred):\n",
    "    from keras import backend\n",
    "    return backend.mean(backend.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "# coefficient of determination (R^2) for regression\n",
    "def r_square(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "\n",
    "def r_square_loss(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return 1 - ( 1 - SS_res/(SS_tot + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "Train on 52479 samples, validate on 5832 samples\n",
      "Epoch 1/1\n",
      "52479/52479 [==============================] - 710s 14ms/step - loss: 0.1081 - mean_squared_error: 0.1081 - rmse: 0.2658 - r_square: -0.7083 - val_loss: 0.0795 - val_mean_squared_error: 0.0795 - val_rmse: 0.2446 - val_r_square: -0.2013\n",
      "[INFO] traing finished\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Compile Keras model\n",
    "#-----------------------------------------------------------------------------\n",
    "# create the Convolutional Neural Network and then compile the model\n",
    "# using mean absolute percentage error as loss, implying that we\n",
    "# seek to minimize the absolute percentage difference between our\n",
    "# adoption speed *predictions* and the *actual adoption speeds*\n",
    "model = create_cnn(64, 64, 3, regress=True)\n",
    "#opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model.compile(optimizer='adam', loss=\"mean_squared_error\", metrics=[\"mean_squared_error\", rmse, r_square])\n",
    "\n",
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "result = model.fit(trainImagesX, trainY, validation_data=(testImagesX, testY),\n",
    "          epochs=1, batch_size=8)\n",
    "print(\"[INFO] traing finished\")\n",
    "\n",
    "# enable early stopping based on mean_squared_error\n",
    "earlystopping=EarlyStopping(monitor=\"mean_squared_error\", patience=40, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading test features...\n",
      "[INFO] processed test features\n",
      "[INFO] loading test images...\n",
      "[INFO] processed test images\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------\n",
    "# Test on Kaggle test set\n",
    "#-----------------------------------------------------------------------------\n",
    "IMAGES_PATH = 'C:/Users/madad/Documents/dataset/images/test/'\n",
    "RESIZED_IMAGES_PATH_TEST = 'C:/Users/madad/Documents/dataset/images/test-resized/'\n",
    "CSV_PATH_TEST = 'C:/Users/madad/Documents/dataset/test.csv'\n",
    "\n",
    "# Load csv\n",
    "print(\"[INFO] loading test features...\")\n",
    "df_test = load_pet_attributes_test(CSV_PATH_TEST)\n",
    "print(\"[INFO] processed test features\")\n",
    "\n",
    "# Load images\n",
    "print(\"[INFO] loading test images...\")\n",
    "images = load_pet_images(df_test, RESIZED_IMAGES_PATH_TEST)\n",
    "images = images / 255.0\n",
    "print(\"[INFO] processed test images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] predicting pet adoption speed...\n",
      "[INFO] predicted pet adoption speed\n",
      "[[0.64105684]\n",
      " [0.6301779 ]\n",
      " [0.6406573 ]\n",
      " ...\n",
      " [0.6355976 ]\n",
      " [0.6072206 ]\n",
      " [0.6484943 ]]\n",
      "--Call--\n",
      "> c:\\users\\madad\\venv\\lib\\site-packages\\ipython\\core\\displayhook.py(247)__call__()\n",
      "-> def __call__(self, result=None):\n",
      "(Pdb) y_pred\n",
      "*** NameError: name 'y_pred' is not defined\n",
      "(Pdb) c\n"
     ]
    }
   ],
   "source": [
    "# make predictions on the testing data\n",
    "print(\"[INFO] predicting pet adoption speed...\")\n",
    "y_pred = model.predict(images)\n",
    "print(\"[INFO] predicted pet adoption speed\")\n",
    "\n",
    "print(y_pred)\n",
    "import pdb; pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "#-----------------------------------------------------------------------------\n",
    "# print statistical figures of merit\n",
    "#-----------------------------------------------------------------------------\n",
    "# print(\"\\n\")\n",
    "# print(\"Mean absolute error (MAE):      %f\" % sklearn.metrics.mean_absolute_error(testY,y_pred))\n",
    "# print(\"Mean squared error (MSE):       %f\" % sklearn.metrics.mean_squared_error(testY,y_pred))\n",
    "# print(\"Root mean squared error (RMSE): %f\" % math.sqrt(sklearn.metrics.mean_squared_error(testY,y_pred)))\n",
    "# print(\"R square (R^2):                 %f\" % sklearn.metrics.r2_score(testY,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Number of pet ids:  15040\n",
      "[INFO] Number of predictions:  15040\n"
     ]
    }
   ],
   "source": [
    "# get df dimensions\n",
    "PetID =np.asarray(df_test['PetID'], dtype = str)\n",
    "n = PetID.shape[0]\n",
    "print('[INFO] Number of pet ids: ', PetID.shape[0])\n",
    "print('[INFO] Number of predictions: ', y_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3820\n",
      "3820\n"
     ]
    }
   ],
   "source": [
    "# parse PetID entries and merge those corresponding to \n",
    "# the same pet by calculating their average\n",
    "import re\n",
    "\n",
    "prev_id = PetID[0].split('-')[0] \n",
    "size = 1\n",
    "sum_avg = 0\n",
    "PetID_fin = []\n",
    "y_pred_fin = []\n",
    "for i in range (1, n):\n",
    "    cur_id = PetID[i].split('-')[0]\n",
    "    if cur_id == prev_id:\n",
    "        sum_avg = sum_avg + y_pred[i]\n",
    "        size = size + 1 \n",
    "    else:\n",
    "        #PetIDs array\n",
    "        PetID_fin.append(prev_id)\n",
    "        # predictions array\n",
    "        y_pred_fin.append(sum_avg/size)\n",
    "        size = 1\n",
    "        sum_avg = 0\n",
    "    prev_id = cur_id\n",
    "    \n",
    "print(len(PetID_fin))\n",
    "print(len(y_pred_fin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write submission file\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "n_fin = len(PetID_fin)\n",
    "\n",
    "with open('mysubmission_Test.csv','w',newline = '') as f:\n",
    "    thewriter = csv.writer(f)\n",
    "    thewriter.writerow(['PetID','AdoptionSpeed'])\n",
    "    for x in range(0, n_fin):\n",
    "        try:\n",
    "            thewriter.writerow([PetID_fin[x], np.round(y_pred_fin[x].item(0)*4)])\n",
    "        except:\n",
    "            thewriter.writerow([PetID_fin[x], np.round(y_pred_fin[x]*4)])\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# Optional: serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
