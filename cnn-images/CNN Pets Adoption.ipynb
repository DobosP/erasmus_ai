{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network\n",
    " \n",
    "# Installing Theano\n",
    "# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n",
    " \n",
    "# Installing Tensorflow\n",
    "# Install Tensorflow from the website: https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html\n",
    " \n",
    "# Installing Keras\n",
    "# pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Part 1 - Building the Models\n",
    " \n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "# Multilayer Perceptron\n",
    "def create_mlp(dim, regress=False):\n",
    "    # define our MLP network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "\n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "    # return our model\n",
    "    return model\n",
    "\n",
    "# Regression-based CNN\n",
    "def create_cnn(width, height, depth, filters=(16, 32, 64), regress=False):\n",
    "    # initialize the input shape and channel dimension, assuming\n",
    "    # TensorFlow/channels-last ordering\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1\n",
    "\n",
    "    # define the model input\n",
    "    inputs = Input(shape=inputShape)\n",
    "\n",
    "    # loop over the number of filters\n",
    "    for (i, f) in enumerate(filters):\n",
    "        # if this is the first CONV layer then set the input\n",
    "        # appropriately\n",
    "        if i == 0:\n",
    "            x = inputs\n",
    "\n",
    "        # Step 1 - Convolution\n",
    "        x = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
    "        \n",
    "        # Step 2 - RELU\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "        # Step 3 - BN\n",
    "        x = BatchNormalization(axis=chanDim)(x)\n",
    "        \n",
    "        # Step 4 - Pooling\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Step 5 - Flattening\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # Step 6 - FC layer\n",
    "    x = Dense(16)(x)\n",
    "    \n",
    "    # Step 7 - RELU\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    # Step 8 - BN\n",
    "    x = BatchNormalization(axis=chanDim)(x)\n",
    "    \n",
    "    # Step 9 - DROPOUT\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Step 10 - apply another FC layer, this one to match the number of nodes\n",
    "    # coming out of the MLP\n",
    "    x = Dense(4)(x)\n",
    "    \n",
    "    # Step 11 - RELU\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        x = Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "    # construct the CNN\n",
    "    model = Model(inputs, x)\n",
    "\n",
    "    # return the CNN\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "\n",
    "def resize_image(path):\n",
    "    image = Image.open(path)\n",
    "    resized_image = image.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "    # Uncomment to save to local directory \n",
    "    # Get image name\n",
    "    name=os.path.basename(path)\n",
    "    resized_image.save('C:/Users/madad/Documents/dataset/images/res1/' + name, \"JPEG\")\n",
    "\n",
    "    # new_image size (64,64)\n",
    "    return np.asarray(resized_image)\n",
    "\n",
    "\n",
    "def read_image(path):\n",
    "    image = Image.open(path)\n",
    "    return np.asarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data helpers\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import glob\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "def process_csv(df):\n",
    "    new_df = pd.DataFrame() \n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        quantity = int(row['PhotoAmt'])\n",
    "        \n",
    "        if (not re.match(\"^[a-zA-Z0-9_]*$\", row['PetID'])):\n",
    "            print(row['PetID'])\n",
    "            continue\n",
    "    \n",
    "        petId = str(row['PetID'])\n",
    "        \n",
    "        for i in range(1, quantity+1):\n",
    "            new_row = row\n",
    "            new_row['PetID'] = petId + '-' + str(i)\n",
    "            \n",
    "            new_df = new_df.append([new_row],ignore_index=True)\n",
    "            \n",
    "    return new_df\n",
    "    \n",
    "def load_pet_attributes(inputPath):\n",
    "    # initialize the list of column names in the CSV file and then\n",
    "    # load it using Pandas\n",
    "    cols = [\"Type\", \"Name\", \"Age\", \"Breed1\", \"Breed2\", \"Gender\", \"Color1\", \"Color2\", \"Color3\", \"MaturitySize\", \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"Health\", \"Quantity\", \"Fee\", \"State\", \"RescuerID\", \"VideoAmt\", \"Description\", \"PetID\", \"PhotoAmt\", \"AdoptionSpeed\"]\n",
    "    df = pd.read_csv(inputPath, header=0, usecols=['PhotoAmt', 'PetID', 'AdoptionSpeed'], names=cols)\n",
    "    \n",
    "    df = process_csv(df)\n",
    "    \n",
    "    # return the data frame\n",
    "    return df\n",
    "\n",
    "def load_pet_images(df, inputDir):    \n",
    "    # initialize images array \n",
    "    images = []\n",
    "    outputImage = np.zeros((64, 64, 3), dtype=\"uint8\")\n",
    "    \n",
    "    # loop over the csv rows\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        img_path = inputDir + row['PetID'] + '.jpg'\n",
    "    \n",
    "        resized = read_image(img_path)\n",
    "        #TODO\n",
    "        outputImage = resized\n",
    "        \n",
    "        # add the image to the set of images the network will be trained on\n",
    "        images.append(outputImage)\n",
    "\n",
    "    # return our set of images\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading pet features...\n",
      "[INFO] processed features\n",
      "[INFO] loading pet images...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (64,64,3) into shape (64,64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6feed079d448>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# range [0, 1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[INFO] loading pet images...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_pet_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'C:/Users/madad/Documents/dataset/images/resized/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[INFO] processed images\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-5bc7b62de807>\u001b[0m in \u001b[0;36mload_pet_images\u001b[1;34m(df, inputDir)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# return our set of images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (64,64,3) into shape (64,64)"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import argparse\n",
    "import locale\n",
    "\n",
    "# construct the path to the train.csv file that contains information\n",
    "# on each pet in the dataset and then load the dataset\n",
    "print(\"[INFO] loading pet features...\")\n",
    "df = load_pet_attributes('C:/Users/madad/Documents/dataset/train.csv')\n",
    "print(\"[INFO] processed features\")\n",
    "\n",
    "# load the pet images and then scale the pixel intensities to the\n",
    "# range [0, 1]\n",
    "print(\"[INFO] loading pet images...\")\n",
    "images = load_pet_images(df, 'C:/Users/madad/Documents/dataset/images/resized/')\n",
    "images = images / 255.0\n",
    "print(\"[INFO] processed images\")\n",
    "\n",
    "# partition the data into training and testing splits using 90% of\n",
    "# the data for training and the remaining 10% for testing\n",
    "split = train_test_split(df, images, test_size=0.1, random_state=42)\n",
    "(trainAttrX, testAttrX, trainImagesX, testImagesX) = split\n",
    "\n",
    "# the largest pet adoption value is 4 \n",
    "# use it to scale the pet adoption speed to the range [0, 1] (will lead to better\n",
    "# training and convergence)\n",
    "maxAdoption = 4\n",
    "trainY = trainAttrX[\"AdoptionSpeed\"] / maxAdoption\n",
    "testY = testAttrX[\"AdoptionSpeed\"] / maxAdoption\n",
    "\n",
    "# create the Convolutional Neural Network and then compile the model\n",
    "# using mean absolute percentage error as loss, implying that we\n",
    "# seek to minimize the absolute percentage difference between our\n",
    "# adoption speed *predictions* and the *actual adoption speeds*\n",
    "model = create_cnn(64, 64, 3, regress=True)\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
    "\n",
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "model.fit(trainImagesX, trainY, validation_data=(testImagesX, testY),\n",
    "          epochs=200, batch_size=8)\n",
    "\n",
    "# make predictions on the testing data\n",
    "print(\"[INFO] predicting pet adoption speed...\")\n",
    "preds = model.predict(testImagesX)\n",
    "\n",
    "# compute the difference between the *predicted* adoption speeds and the\n",
    "# *actual* adoption speeds, then compute the percentage difference and\n",
    "# the absolute percentage difference\n",
    "diff = preds.flatten() - testY\n",
    "percentDiff = (diff / testY) * 100\n",
    "absPercentDiff = np.abs(percentDiff)\n",
    "\n",
    "# compute the mean and standard deviation of the absolute percentage\n",
    "# difference\n",
    "mean = np.mean(absPercentDiff)\n",
    "std = np.std(absPercentDiff)\n",
    "\n",
    "# finally, show some statistics on our model\n",
    "locale.setlocale(locale.LC_ALL, \"en_US.UTF-8\")\n",
    "print(\"[INFO] avg. house price: {}, std house price: {}\".format(\n",
    "    locale.currency(df[\"AdoptionSpeed\"].mean(), grouping=True),\n",
    "    locale.currency(df[\"AdoptionSpeed\"].std(), grouping=True)))\n",
    "print(\"[INFO] mean: {:.2f}%, std: {:.2f}%\".format(mean, std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "classifier.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
